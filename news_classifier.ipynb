{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DEV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data\n",
      "\n",
      "Splitting data\n",
      "Training:  1779\n",
      "Developement:  594\n",
      "Testing:  792\n",
      "\n",
      "Vectorizing data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applyting Feature Reduction\n",
      "Number of features before reduction :  4303\n",
      "Number of features after reduction :  1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training baseline classifier\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.11      0.19      0.14        73\n",
      "  Criminal Justice       0.09      0.12      0.10        77\n",
      "       Health Care       0.09      0.20      0.12        60\n",
      " Politics & Policy       0.39      0.17      0.24       259\n",
      "  Science & Health       0.18      0.17      0.17       125\n",
      "\n",
      "          accuracy                           0.17       594\n",
      "         macro avg       0.17      0.17      0.15       594\n",
      "      weighted avg       0.24      0.17      0.18       594\n",
      "\n",
      "Training Decision tree\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.29      0.52      0.37        73\n",
      "  Criminal Justice       0.41      0.51      0.46        77\n",
      "       Health Care       0.38      0.52      0.44        60\n",
      " Politics & Policy       0.75      0.54      0.62       259\n",
      "  Science & Health       0.60      0.49      0.54       125\n",
      "\n",
      "          accuracy                           0.52       594\n",
      "         macro avg       0.49      0.51      0.49       594\n",
      "      weighted avg       0.58      0.52      0.53       594\n",
      "\n",
      "Training Random Forest\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.32      0.47      0.38        73\n",
      "  Criminal Justice       0.40      0.56      0.46        77\n",
      "       Health Care       0.36      0.52      0.42        60\n",
      " Politics & Policy       0.77      0.54      0.64       259\n",
      "  Science & Health       0.63      0.55      0.59       125\n",
      "\n",
      "          accuracy                           0.54       594\n",
      "         macro avg       0.49      0.53      0.50       594\n",
      "      weighted avg       0.59      0.54      0.55       594\n",
      "\n",
      "Training Multinomial Naive Bayesian\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.44      0.55      0.49        73\n",
      "  Criminal Justice       0.51      0.51      0.51        77\n",
      "       Health Care       0.44      0.53      0.48        60\n",
      " Politics & Policy       0.73      0.65      0.69       259\n",
      "  Science & Health       0.65      0.66      0.65       125\n",
      "\n",
      "          accuracy                           0.61       594\n",
      "         macro avg       0.56      0.58      0.56       594\n",
      "      weighted avg       0.62      0.61      0.61       594\n",
      "\n",
      "Training Support Vector Classification\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.31      0.26      0.28        73\n",
      "  Criminal Justice       0.76      0.21      0.33        77\n",
      "       Health Care       0.59      0.27      0.37        60\n",
      " Politics & Policy       0.55      0.90      0.69       259\n",
      "  Science & Health       0.73      0.36      0.48       125\n",
      "\n",
      "          accuracy                           0.56       594\n",
      "         macro avg       0.59      0.40      0.43       594\n",
      "      weighted avg       0.59      0.56      0.51       594\n",
      "\n",
      "Training Multilayered Perceptron\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.43      0.55      0.48        73\n",
      "  Criminal Justice       0.48      0.51      0.49        77\n",
      "       Health Care       0.42      0.45      0.43        60\n",
      " Politics & Policy       0.72      0.65      0.68       259\n",
      "  Science & Health       0.62      0.59      0.61       125\n",
      "\n",
      "          accuracy                           0.59       594\n",
      "         macro avg       0.53      0.55      0.54       594\n",
      "      weighted avg       0.60      0.59      0.59       594\n",
      "\n",
      "\n",
      "\n",
      "Predicting test data using Multinomial Naive Bayesian\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Business & Finance       0.48      0.59      0.53       110\n",
      "  Criminal Justice       0.50      0.52      0.51        87\n",
      "       Health Care       0.49      0.58      0.53        89\n",
      " Politics & Policy       0.73      0.63      0.68       334\n",
      "  Science & Health       0.65      0.65      0.65       172\n",
      "\n",
      "          accuracy                           0.61       792\n",
      "         macro avg       0.57      0.59      0.58       792\n",
      "      weighted avg       0.63      0.61      0.62       792\n",
      "\n",
      "\n",
      "\n",
      "Incorrectly classified\n",
      "\n",
      "Title:  the dow topped 18,000 today learn why that matters. \n",
      "True Category:  Business & Finance \n",
      "Predicted Category:  Health Care\n"
     ]
    }
   ],
   "source": [
    "# ## Import Libraries\n",
    "# Loading all libraries to be used\n",
    "import copy\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import accuracy_score\n",
    "#from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# # Data preparation\n",
    "# ## Load data\n",
    "titles = []\n",
    "categories = []\n",
    "print(\"\\nLoading data\")\n",
    "with open('dsjVoxArticles.tsv','r',encoding='utf-8') as tsv:\n",
    "    count = 0;\n",
    "    for line in tsv:\n",
    "        a = line.strip().split('\\t')[:3]\n",
    "        if a[2] in ['Business & Finance', 'Health Care', 'Science & Health', 'Politics & Policy', 'Criminal Justice']:\n",
    "            title = a[0].lower()\n",
    "            title = re.sub('\\s\\W',' ',title)\n",
    "            title = re.sub('\\W\\s',' ',title)\n",
    "            titles.append(title)\n",
    "            categories.append(a[2])\n",
    "\n",
    "# ## Split data\n",
    "print(\"\\nSplitting data\")\n",
    "title_tr, title_te, category_tr, category_te = train_test_split(titles,categories)\n",
    "title_tr, title_de, category_tr, category_de = train_test_split(title_tr,category_tr)\n",
    "print(\"Training: \",len(title_tr))\n",
    "print(\"Developement: \",len(title_de),)\n",
    "print(\"Testing: \",len(title_te))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Data Preprocessing\n",
    "# ## Vectorization of data\n",
    "# Vectorize the data using Bag of words (BOW)\n",
    "print(\"\\nVectorizing data\")\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r\"\\w+\")\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "vectorizer = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stop_words)\n",
    "\n",
    "vectorizer.fit(iter(title_tr))\n",
    "Xtr = vectorizer.transform(iter(title_tr))\n",
    "Xde = vectorizer.transform(iter(title_de))\n",
    "Xte = vectorizer.transform(iter(title_te))\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(category_tr)\n",
    "Ytr = encoder.transform(category_tr)\n",
    "Yde = encoder.transform(category_de)\n",
    "Yte = encoder.transform(category_te)\n",
    "\n",
    "# ## Feature Reduction\n",
    "# We can check the variance of the feature and drop them based on a threshold\n",
    "print(\"\\nApplyting Feature Reduction\")\n",
    "print(\"Number of features before reduction : \", Xtr.shape[1])\n",
    "selection = VarianceThreshold(threshold=0.001)\n",
    "Xtr_whole = copy.deepcopy(Xtr)\n",
    "Ytr_whole = copy.deepcopy(Ytr)\n",
    "selection.fit(Xtr)\n",
    "Xtr = selection.transform(Xtr)\n",
    "Xde = selection.transform(Xde)\n",
    "Xte = selection.transform(Xte)\n",
    "print(\"Number of features after reduction : \", Xtr.shape[1])\n",
    "\n",
    "# ## Sampling data\n",
    "sm = SMOTE(random_state=42)\n",
    "Xtr, Ytr = sm.fit_sample(Xtr, Ytr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Train Models\n",
    "# ### Baseline Model\n",
    "# “stratified”: generates predictions by respecting the training set’s class distribution.\n",
    "print(\"\\n\\nTraining baseline classifier\")\n",
    "dc = DummyClassifier(strategy=\"stratified\")\n",
    "dc.fit(Xtr, Ytr)\n",
    "pred = dc.predict(Xde)\n",
    "print(classification_report(Yde, pred, target_names=encoder.classes_))\n",
    "\n",
    "# ### Decision Tree\n",
    "print(\"Training Decision tree\")\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(Xtr, Ytr)\n",
    "pred = dt.predict(Xde)\n",
    "print(classification_report(Yde, pred, target_names=encoder.classes_))\n",
    "\n",
    "# ### Random Forest\n",
    "print(\"Training Random Forest\")\n",
    "rf = RandomForestClassifier(n_estimators=40)\n",
    "rf.fit(Xtr, Ytr)\n",
    "pred = rf.predict(Xde)\n",
    "print(classification_report(Yde, pred, target_names=encoder.classes_))\n",
    "\n",
    "# ### Multinomial Naive Bayesian\n",
    "print(\"Training Multinomial Naive Bayesian\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(Xtr, Ytr)\n",
    "pred_nb = nb.predict(Xde)\n",
    "print(classification_report(Yde, pred_nb, target_names=encoder.classes_))\n",
    "\n",
    "# ### Support Vector Classification\n",
    "print(\"Training Support Vector Classification\")\n",
    "from sklearn.svm import SVC\n",
    "svc = SVC()\n",
    "svc.fit(Xtr, Ytr)\n",
    "pred = svc.predict(Xde)\n",
    "print(classification_report(Yde, pred, target_names=encoder.classes_))\n",
    "\n",
    "# ### Multilayered Perceptron\n",
    "print(\"Training Multilayered Perceptron\")\n",
    "mlp = MLPClassifier(solver='adam', alpha=1e-5, hidden_layer_sizes=(100, 20), random_state=1, max_iter=400)\n",
    "mlp.fit(Xtr, Ytr)\n",
    "pred = mlp.predict(Xde)\n",
    "print(classification_report(Yde, pred, target_names=encoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Final Model: Multinomial Naive Bayesian\n",
    "# **Multinomial Naive Bayesian** works the best. Lets run NB on our test data and get the confusion matrix and its heat map.\n",
    "# ## Predict test data\n",
    "print(\"\\n\\nPredicting test data using Multinomial Naive Bayesian\")\n",
    "pred_final = nb.predict(Xte)\n",
    "print(classification_report(Yte, pred_final, target_names=encoder.classes_))\n",
    "\n",
    "\n",
    "# get incorrectly classified data\n",
    "print(\"\\n\\nIncorrectly classified\")\n",
    "incorrect = np.not_equal(pred_nb, Yde).nonzero()[0]\n",
    "print(\n",
    "    \"\\nTitle: \",titles[incorrect[6]],\n",
    "    \"\\nTrue Category: \",categories[incorrect[6]],\n",
    "    \"\\nPredicted Category: \", encoder.inverse_transform([pred[incorrect[6]]])[0]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

